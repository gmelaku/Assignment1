{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PS3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmelaku/Assignment1/blob/master/PS3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vamZ0iOZf0i-",
        "colab_type": "text"
      },
      "source": [
        "# Homework-3\n",
        "## Question-6: Fill out the right hand side and/or below #TODO comments \n",
        "\n",
        "1. Setup the system and environment variables\n",
        "\n",
        "2. You need to work on the Project Gutenberg edition of Jane Austenâ€™s Pride and Prejudice (https://www.gutenberg.org/files/1342/1342-0.txt)\n",
        "\n",
        "3. Sort the unique words in the book based on both words and counts in assending and descending order.\n",
        "\n",
        "3. Split the 61 chapters (dataframe) into 6 bins of 10 chapters in each: Ch-1 to Ch-10, Ch-11 to Ch-20, Ch-21 to Ch-30, Ch-31 to Ch-40, Ch-41 to Ch-50, Ch-51 to End. The code to split the book in to a dataframe with chapter-number and text column is given.\n",
        "\n",
        "4. Randomly shuffle the dataframe containing 61 chapters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keq9Gd-8S5gD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs_jEB3gTH06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEvvrIzQjjeM",
        "colab_type": "text"
      },
      "source": [
        "## Sorting: orderBy() function is described at https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
        "- Load the text file using SparkContext\n",
        "- get the word counts RDD\n",
        "- Conver word counts RDD to dataframe and print 5 entries\n",
        "- Sort dataframe by words and counts in assending order and print 5 entries for both\n",
        "- Sort dataframe by words and counts in descending order and print 5 entries for both"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogwDuKFnY__O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import desc\n",
        "# TODO: specify the path to the input file (String)\n",
        "ipFilePath = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
        "words = sc.textFile(ipFilePath).flatMap(lambda line: line.split(\" \"))\n",
        "print (type(words))\n",
        "words.collect()\n",
        "\n",
        "\n",
        "# get word-count RDD\n",
        "words.count()\n",
        "#wordCounts = words.count() \n",
        "#print (type(wordCounts))\n",
        "#print wordCounts.take(20)\n",
        "\n",
        "# Convert word-count RDD to dataframe with 2 columns [\"word\",\"count\"]\n",
        "# TODO\n",
        "wcDF = words.toDF([\"word\", \"count\"]) \n",
        "print (type(wcDF))\n",
        "wcDF.show(5)\n",
        "\n",
        "# sorting in assending order\n",
        "\n",
        "wcDF.count().filter(\"count\" >= 10\").sort(desc(\"count\"))\n",
        "print ('sorted by word: descending')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Tm_GrjlLfo",
        "colab_type": "text"
      },
      "source": [
        "## Binning: Sample example at https://github.com/apache/spark/blob/master/examples/src/main/python/ml/bucketizer_example.py\n",
        "- Convert a list of tuples as (chapter-num, chapter-text) into a dataframe and print 5 entries \n",
        "- Use pyspark.ml.feature.Bucketizer for splitting a dataframe into diferent buckets:  Ch-1 to Ch-10, Ch-11 to Ch-20, Ch-21 to Ch-30, Ch-31 to Ch-40, Ch-41 to Ch-50, Ch-51 to End. \n",
        "- Print 20 entries to check the buckets assignments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpxAZMCyTMPB",
        "colab_type": "code",
        "outputId": "e1b07b13-2ddf-4995-fb39-5af1f24dfd90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "import unicodedata\n",
        "import re\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "################## Code for getting chapters out of book #####################\n",
        "def normalize_text(text):\n",
        "    if not isinstance(text, unicode):\n",
        "        text = unicode(text, 'utf-8')\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore') \n",
        "    return text\n",
        "\n",
        "def remove_white_spaces(text):\n",
        "    text = re.sub('\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "with open('/content/sample_data/1342-0.txt', 'r') as content_file:\n",
        "    content = content_file.read()\n",
        "\n",
        "content = remove_white_spaces(normalize_text(content))\n",
        "\n",
        "start_str = 'Produced by Anonymous Volunteers'\n",
        "end_str = 'End of the Project Gutenberg EBook of Pride and Prejudice, by Jane Austen'\n",
        "actual_content = content[content.find(start_str)+len(start_str):content.rfind(end_str)]\n",
        "\n",
        "chapters = re.split(\" Chapter [0-9]+ \", actual_content, flags=re.IGNORECASE)[1:]\n",
        "\n",
        "ch_data = []\n",
        "for i in range(1,1+len(chapters)):\n",
        "    ch_data.append((i,chapters[i-1]))\n",
        "##############################################################################\n",
        "\n",
        "# Get the dataframe out of list of tuples with columns as [\"chapter\", \"text\"]\n",
        "# TODO\n",
        "df = \n",
        "df.show(5)\n",
        "\n",
        "# do the binning using Bucketizer\n",
        "# TODO\n",
        "bucketizer = \n",
        "df_buck = \n",
        "\n",
        "df_buck.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------+\n",
            "|chapter|                text|\n",
            "+-------+--------------------+\n",
            "|      1|It is a truth uni...|\n",
            "|      2|Mr. Bennet was am...|\n",
            "|      3|Not all that Mrs....|\n",
            "|      4|When Jane and Eli...|\n",
            "|      5|Within a short wa...|\n",
            "+-------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+--------------------+-------+\n",
            "|chapter|                text|buckets|\n",
            "+-------+--------------------+-------+\n",
            "|      1|It is a truth uni...|    0.0|\n",
            "|      2|Mr. Bennet was am...|    0.0|\n",
            "|      3|Not all that Mrs....|    0.0|\n",
            "|      4|When Jane and Eli...|    0.0|\n",
            "|      5|Within a short wa...|    0.0|\n",
            "|      6|The ladies of Lon...|    0.0|\n",
            "|      7|Mr. Bennet's prop...|    0.0|\n",
            "|      8|At five o'clock t...|    0.0|\n",
            "|      9|Elizabeth passed ...|    0.0|\n",
            "|     10|The day passed mu...|    0.0|\n",
            "|     11|When the ladies r...|    1.0|\n",
            "|     12|In consequence of...|    1.0|\n",
            "|     13|I hope, my dear, ...|    1.0|\n",
            "|     14|During dinner, Mr...|    1.0|\n",
            "|     15|Mr. Collins was n...|    1.0|\n",
            "|     16|As no objection w...|    1.0|\n",
            "|     17|Elizabeth related...|    1.0|\n",
            "|     18|Till Elizabeth en...|    1.0|\n",
            "|     19|The next day open...|    1.0|\n",
            "|     20|Mr. Collins was n...|    1.0|\n",
            "+-------+--------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU6lDY4lmOQn",
        "colab_type": "text"
      },
      "source": [
        "## Shuffling: https://stackoverflow.com/questions/43637625/how-to-shuffle-the-rows-in-a-spark-dataframe\n",
        "- Print 5 entries of the dataframe containing all 61 chapters\n",
        "- Use pyspark.sql.functions.rand function for randomly shuffling the dataframe containing all 61 chapters and print 5 entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXZZdv-Mh14t",
        "colab_type": "code",
        "outputId": "178d89ef-1513-4cc5-e200-d0ef85a52e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "from pyspark.sql.functions import rand \n",
        "# print 5 entries of the dataframe containing all 61 chapters\n",
        "# TODO\n",
        "\n",
        "\n",
        "# randomly shuffle the dataframe and print 5 entries\n",
        "# TODO\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------+\n",
            "|chapter|                text|\n",
            "+-------+--------------------+\n",
            "|      1|It is a truth uni...|\n",
            "|      2|Mr. Bennet was am...|\n",
            "|      3|Not all that Mrs....|\n",
            "|      4|When Jane and Eli...|\n",
            "|      5|Within a short wa...|\n",
            "+-------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+--------------------+\n",
            "|chapter|                text|\n",
            "+-------+--------------------+\n",
            "|      4|When Jane and Eli...|\n",
            "|     14|During dinner, Mr...|\n",
            "|      7|Mr. Bennet's prop...|\n",
            "|     27|With no greater e...|\n",
            "|     34|When they were go...|\n",
            "+-------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}